\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{lipsum}
\usepackage{multicol}

\title{Adversarial Cross-lingual Context Vectors}
\author{Nicholas Farn}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
	\lipsum[1]
\end{abstract}

\begin{multicols}{2}
\section{Introduction}
\lipsum[1]

\section{Previous Work}
\lipsum[1]

\section{Model}
The cross-lingual model consists of using a transformer architecture on the shared-private multi-task model architecture like in Chen et al.  This consists of a shared transformer for every language along with private transformers for each individual language.

The BERT model implementation is taken from a repository verified to reproduce the same results as the original paper.  Additional fragments of code such as the vocab class are taken from another repository implementing BERT.

I implemented the shared-private model, which encapsulates a BERT transformer model and outputs context vectors for a specific language given input token ids.  Tokens across languages are shared in a single multilingual vocabulary.

\subsection{BERT and ELMO}
\lipsum[1]

\subsection{Shared-Private Model}
\lipsum[1]

\section{Training}
I also implemented a potential pretraining algorithm which consists of training an adversary to recognize the language an output representation belongs to, then performing a modified version of BERT’s prediction tasks for each language.

BERT’s pre-training consists of two tasks: predicting the correct token for randomly masked tokens in a sequence and predicting whether a given sentence follows a given sentence.  The first task is generated by having a probability of a masked token either being masked, replaced with a random token, or kept the same.  I modified the algorithm to include a probability of replacing a given token with a context equivalent token in another language. This should hopefully encourage similar representations across different languages though I do not currently have empirical evidence.  This also requires the use of token-aligned parallel corpus.

Each language specific model is also trained to confuse an adversary while also minimizing the squared frobenius loss between its shared and private vectors.  Thus the total loss is a combination of the prediction loss from BERT’s tasks plus the adversary’s prediction loss and frobenius loss.  The latter two losses are weighted through the beta and gamma hyperparameters.

$language_loss = mask_loss + is_next_loss + beta * adversary_loss + gamma * frobenius_loss$

\section{Evaluation}
Like BERT, training data consists of long sequences of sentences drawn from texts such as news articles or books.  Samples sequences are drawn for each language from a set of corpora belonging to the specified language, creating a training set for each language.  If the corpus a sample is drawn from consists of parallel texts, the accompanying translation for a sequence is included in the sample as well.  In addition to drawing samples for each language, samples are drawn from all corpora across all languages in order to create a training set for the adversary.

\subsection{XNLI}
\lipsum[1]

\subsection{Results}
\lipsum[1]

\section{Conclusion}
\lipsum[1]

\nocite{*}
\bibliography{project_report}
\bibliographystyle{plain}
\end{multicols}
\end{document}