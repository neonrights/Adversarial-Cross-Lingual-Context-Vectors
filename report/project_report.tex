\documentclass[10pt,letterpaper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{lipsum}

\title{Adversarial Cross-lingual Language Models}
\author{Farn, Nicholas\\
	\texttt{Department of Computer Science,}\\
	\texttt{University of California, Los Angeles}\\
	\texttt{nfarn@cs.ucla.edu}
}

\begin{document}
\maketitle
\begin{abstract}
Write after everything else
\end{abstract}

\section{Introduction}
Many natural language processing tasks commonly make use of pre-trained models, such as word embeddings, to much success \cite{}.  Such pre-trained models often create a vector representation of words that impart some meaning in relation to vector representations of other words within the same language.  Recently, such models have shown great improvement beyond state-of-the-art through unsupervised training of deep language models \cite{bert, openai, elmo}.  These generate word embeddings altered to take in the surrounding context of the sentence it is in.  However, most models training tasks are designed to model a single language, and thus similar words and sentences in different languages can have vastly different representations.  Aligning sentences using bilingual corpora can take a vast amount of samples.  This is on top of the many samples needed in order to produce good representations in a given language.  For lower resource languages such as Swahili or Urdu, this can pose a problem.  

There has been previous success in jointly training multiple tasks with similar tasks on a single model.

\section{Related Work}
Facebook stop stealing my research :P

\section{Model}
Like previous models, the adversarial crosslingual language model (AXLM) operates by producing word embeddings for each token in an input sentence or sequence of sentences \cite{bert, openai, elmo, xlm}.  Word embeddings are produced for a specific input language by concatenating the outputs of two language models, one shared between all languages and one unique to the input language.  Given $L$ languages, such a model ends up jointly training $L+1$ language models.  This multi-task set-up is commonly referred to as a shared-private encoder \cite{Chen16}.  The goal of using such an architecture is to leverage data for higher resource languages to learn useful features for lower resource languages.  The model is split into shared and private components in order to prevent competition for model parameters between languages.  An adversarial component is added in order to encourage similar representation between languages as well.

% image showing example of network

\subsection{Language Model}
AXLM uses a transformer architecture on the shared-private architecture.  The transformer is trained on monolingual data using the same input and prediction tasks as BERT.  Input to BERT consists of first tokenizing sentences using WordPiece tokenization, the prepending and appending a [CLS] and [SEP] token to the sentence respectively.  BERT training consists of two tasks prediction:

\begin{itemize}
	\item Masked token prediction.  Given a sequence of tokens, $(t_1, t_2, t_3, \dots)$, a given token, $t_i$, is chosen to be replaced with a special ``mask'' token with a probability of 15\%.  The language model is then trained to predict the masked word.  Since the mask token is never seen in downstream tasks, it is replaced with a random word 10\% of the time, the original word is kept another 10\% of the time, and the mask token is used for the remaining 80\%.

	\item Next sentence prediction.  The model is given two sentences which are concatenated together.  50\% of the time, the second sentence is an actual sentence which follows the first.  The other 50\% it is a sentence randomly drawn from the corpus.  In order to aid the language model, the sentences are separated by a [SEP] token.  Each sentence is also given a special segment embedding, with each token in the first sentence given a first sentence segment embedding and each token in the second receiving a corresponding second sentence segment embedding.
\end{itemize}

During training the loss from each task is simply added together.  In addition to segment embeddings, BERT also makes use of positional and multilingual Word-piece embeddings \cite{}.  For AXLM, these embeddings are shared between all private and shared components.

% picture of embedding setup

\subsection{Shared-Private Model}
In addition to the loss from BERT's prediction tasks, AXLM is also subjected to an adversarial and Frobenius loss.  Since the shared language model is the same for all languages, it is encouraged to learn language agnostic.  This is accomplished by training an adversarial discriminator to predict the language an input sentence belongs.  The discriminator is defined as a simple prediction layer which is fed the pooled output of the shared model.

$$D(x_{pooled}) = softmax(W x_{pooled} + b)$$

The shared language model is then trained to confuse the discriminator.

$$\mathcal{L}_{adv} = \min_\theta \left( \max_\theta ( \sum_{k=1}^K \sum_{i=1}^N d_i^k \log [D(E(s))] ) \right)$$

Where $E$ and $D$ refer to the AXLM encoder and discriminator respectively.  In order to encourage each language's private models to learn language specific features, the squared Frobenius loss is taken between the output vectors of a language's private and shared components.

$$\mathcal{L}_{diff} = \sum_{k=1}^K \lVert {H_s^k}^T H_p^k \rVert_F^2$$

This penalizes the shared and private components for learning features which are not orthogonal to one another.  The final resultant loss is a weighted combination of the training task, adversarial, and squared Frobenius loss.

$$\mathcal{L} = \mathcal{L}_{mask} + \mathcal{L}_{next} + \beta \mathcal{L}_{adv} + \gamma \mathcal{L}_{diff}$$

Where $\beta$ and $\gamma$ are hand-tuned hyperparameters.  These are chosen to be in the range of 1e-2 to 1e-6.

\section{Training}
AXLM was trained using data drawn from Wikipedia articles using WikiExtractor\footnote{https://github.com/attardi/wikiextractor} and the OpenSubtitles corpus available on OPUS\footnote{http://opus.nlpl.eu/}.  In order to generate training samples, long sequences of sentences are randomly sampled from contiguous text from the corpora.  When generating a single batch, samples are drawn from all languages.  A sample is chosen to be from a given language $L$ with probability $\Pr\{L\}$, total samples from $L$ out of all samples, downsampled by some factor $F$ which is then normalized.  When $F < 1$, this results in higher resource languages being downsampled, giving more attention to lower resource languages.  There is a uniform probability of drawing a sample from a given language in order to ensure all samples are seen \cite{}.

Before training the shared and private language models, the discriminator is first trained for several iterations in order to provide a more robust adversary.  More details about implementation specifics can be viewed in the appendix.  Due to a lack of resources, each language model was initialized using the weights from a pretrained pytorch implementation of multilingual BERT\footnote{https://github.com/huggingface/pytorch-pretrained-BERT}\footnote{https://github.com/codertimo/BERT-pytorch}.

Due to the already large size of BERT, AXLM is even larger, and grows linearly in size with number of languages considered.  However, this is not a problem during forward and backward propagation since only the shared and a single private language model are used at a time.  However, depending on the optimizer used, the linear increase in weights can lead to hefty memory costs when using optimizers such as Adam which uses twice as much memory as the model weights.  For this reason Adafactor was used instead \cite{}.  Best settings used during optimization.  Any kind of corrections used as well.

\section{Evaluation}
After pretraining, AXLM was tuned for a translation task.  Its performance is evaluated through the use of the XNLI dataset \cite{}.  Due to the larger number of parameters available to AXLM compared to other competing crosslingual language models, the shared language model was separated and its performance evaluated by itself as well.  Several smaller models were also developed to provide guidance on the choice of setting $\beta$ and $\gamma$, whose results will be discussed.

\subsection{Results and Analysis}

\section{Conclusion}

\nocite{*}
\bibliography{project_report}
\bibliographystyle{plain}

\section{Implementation}
pseudo-code for training and fine-tuning algorithms
details of data generation and sampling
details of devices used and time/epochs taken to train
\end{document}