# Language Adversarial Training for Cross-lingual Contextualized Vectors

## Introduction

Implementation of multi-task shared-private model in order to create context dependent cross-lingual representations of sentences.  Model jointly performs BERT's pretraining task for a set of languages and their corresponding corpora.  BERT's masked token prediction task is modified to have a probability of swapping an input token with a context equivalent token in another language.  This is in addition to thhe probability of swapping an input token with the mask token or a random token.

## Usage

### Data generation

Data is generated by sampling long sequences of sentences from various books and documents.  Given a supported corpus, data can be generated by calling the following command

```bash
crosslingual_bert corpus [-h] -c CORPUS_CONFIG [--max-length MAX_LENGTH]
                         [-o OUTPUT] [-r] [-s SAMPLES]
```

The script generates training and test data for each language and the adversary.  Training and test data can be drawn from multiple corpora.  Configuration for what corpus belongs to what language is specified by a json config file.

```json
// sample corpus configuration file
{
	"language1": ["supported_corpus_name1", "supported_corpus_name2"],
	"language2": ["supported_corpus_name3"],
	"language3": ["supported_corpus_name4"]
}
```

Unsupported corpora can be added by extending the CorpusReader class [details](corpus/README.md).

### Pretraining

Pretraining can be done by invoking the following command.

```bash
python crosslingual_bert pretrain [-h] [-b BATCH] [-v VOCAB] --dataset-config
                         DATASET_CONFIG [--layers LAYERS] [--hidden HIDDEN]
                         [--intermediate INTERMEDIATE]
                         [--max-seq-len MAX_SEQ_LEN] [--heads HEADS]
                         [--dropout DROPOUT] [--lr LR] [--loss-beta LOSS_BETA]
                         [--loss-gamma LOSS_GAMMA] [--epochs EPOCHS]
                         [--checkpoint CHECKPOINT] [--save-freq SAVE_FREQ]
```

### Task Evaluation

In progress, will crash when invoked

```bash
python crosslingual_bert evaluate [-h] -m MODEL [-t TEST] [-o OUTPUT]
```

## Data

Test scripts currently use the czech-english word aligned corpus available [here](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1804).

## Contributing

Extending data generation, corpus readers, and tokenizers. [link](crosslingual_bert/corpus/README.md)

Extending vocab and pytorch datasets. [link](crosslingual_bert/dataset/README.md)

Extending embeddings, attention, and shared-private models. [link](crosslingual_bert/model/README.md)

Extending pretraining tasks and optimizers. [link](crosslingual_bert/trainer/README.md)

## Coding References
[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)
[pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)
