# Language Adversarial Training for Cross-lingual Contextualized Vectors

## Introduction

Implementation of multi-task shared-private model in order to create context dependent cross-lingual representations of sentences.  Model jointly performs BERT's pretraining task for a set of languages and their corresponding corpora.  BERT's masked token prediction task is modified to have a probability of swapping an input token with a context equivalent token in another language.  This is in addition to thhe probability of swapping an input token with the mask token or a random token.

## Usage

### Data generation

Data is generated by sampling long sequences of sentences from various books and documents.  Given a supported corpus, data can be generated by calling the following command:

```bash
crosslingual_bert corpus --corpus corpus.config
```

Further optional flags can be found by invoking the ```--help``` flag. The script generates data for each language and the adversary which consists of lines of json containing sequences of tokenized sentences.  Data can be drawn from multiple corpora.  Configuration for what corpus belongs to what language is specified by a json config file.

```json
// sample corpus configuration file
{
	"language1": ["supported_corpus_name1", "supported_corpus_name2"],
	"language2": ["supported_corpus_name3"],
	"language3": ["supported_corpus_name4"]
}
```

Unsupported corpora can be added by extending the CorpusReader class [details](https://github.com/neonrights/Adversarial-Cross-Lingual-Context-Vectors/tree/adversarial/crosslingual_bert/corpus).

### Pretraining

Pretraining can be done by invoking the following command.

```bash
crosslingual_bert pretrain --train-data train.config --test-data test-config
```

Further optional flags can be found by invoking the ```--help``` flag. Like data generation, pretraining requires a configuration file for the training and test data sets for each language and the adversary.

```json
// sample data configuration file
{
     "adversary": "adversary_data.txt",
     "language1": "language1_data.txt",
     "language2": "language2_data.txt",
}
```

Pretraining consists of first training the adversary for a set number of cycles before then training each language model.  Each language model is trained using BERT's pretraining task an is optimized using a weighted sum of BERT's task specific losses, adversary prediction loss, and the squared Frobenius distance between shared and private features.

### Task Evaluation

In progress, will crash when invoked.

```bash
python crosslingual_bert evaluate [-h] -m MODEL [-t TEST] [-o OUTPUT]
```

## Data

Test scripts currently use the czech-english word aligned corpus available [here](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1804).

## Contributing

Extending data generation, corpus readers, and tokenizers. [link](crosslingual_bert/corpus/README.md)

Extending vocab and pytorch datasets. [link](crosslingual_bert/dataset/README.md)

Extending embeddings, attention, and shared-private models. [link](crosslingual_bert/model/README.md)

Extending pretraining tasks and optimizers. [link](crosslingual_bert/trainer/README.md)

## Coding References
[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)

[pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)
