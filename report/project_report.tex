\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{lipsum}
\usepackage{multicol}

\title{Adversarial Cross-lingual Context Vectors}
\author{Nicholas Farn}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
Write after everything else
\end{abstract}

\begin{multicols}{2}
\section{Introduction}
Many natural language processing tasks make use of pre-trained models such as word-embeddings \cite{}.  Such pre-trained models often create a representation of words, characters, or sentences that impart some meaning in relation to other words, characters, or sentences within the same language.  Recently, such models have shown vast improvement beyond state-of-the-art through unsupervised training of deep language models \cite{BERT, OpenAI, ELMo}.  However, an inherent limitation is that a unique model must be trained for each unique language used in a downstream application.  Due to the large data required in order to train these models, their performance naturally suffers on lower resource languages such as Swahili or Urdu.  

Representation is not aligned between languages

Short description of solution and logic behind solution

\section{Previous Work}
Describe previous language models

Describe previous types of attempt at aligning language models

Describe previous usage of shared-private models and how it is novel and differs
from other attempts

\section{Model}
The cross-lingual model consists of using a transformer architecture on the shared-private multi-task model architecture like in Chen et al.  This consists of a shared transformer for every language along with private transformers for each individual language.

The BERT model implementation is taken from a repository verified to reproduce the same results as the original paper.  Additional fragments of code such as the vocab class are taken from another repository implementing BERT.

I implemented the shared-private model, which encapsulates a BERT transformer model and outputs context vectors for a specific language given input token ids.  Tokens across languages are shared in a single multilingual vocabulary.

\subsection{Language Model}
Description of language models chosen and baseline model architecture and loss

\subsection{Shared-Private Model}
Description of shared-private model architecture and loss

I also implemented a potential pretraining algorithm which consists of training an adversary to recognize the language an output representation belongs to, then performing a modified version of BERT’s prediction tasks for each language.

BERT’s pre-training consists of two tasks: predicting the correct token for randomly masked tokens in a sequence and predicting whether a given sentence follows a given sentence.  The first task is generated by having a probability of a masked token either being masked, replaced with a random token, or kept the same.  I modified the algorithm to include a probability of replacing a given token with a context equivalent token in another language. This should hopefully encourage similar representations across different languages though I do not currently have empirical evidence.  This also requires the use of token-aligned parallel corpus.

Each language specific model is also trained to confuse an adversary while also minimizing the squared frobenius loss between its shared and private vectors.  Thus the total loss is a combination of the prediction loss from BERT’s tasks plus the adversary’s prediction loss and frobenius loss.  The latter two losses are weighted through the beta and gamma hyperparameters.

$language_loss = mask_loss + is_next_loss + beta * adversary_loss + gamma * frobenius_loss$

\section{Evaluation}
Like BERT, training data consists of long sequences of sentences drawn from texts such as news articles or books.  Samples sequences are drawn for each language from a set of corpora belonging to the specified language, creating a training set for each language.  If the corpus a sample is drawn from consists of parallel texts, the accompanying translation for a sequence is included in the sample as well.  In addition to drawing samples for each language, samples are drawn from all corpora across all languages in order to create a training set for the adversary.

\subsection{XNLI}
used model as embeddings in downstream translation application

\subsection{Results}

\section{Conclusion}

\nocite{*}
\bibliography{project_report}
\bibliographystyle{plain}

\section{Implementation}
pseudo-code for training and fine-tuning algorithms
details of data generation and sampling
details of devices used and time/epochs taken to train
\end{multicols}
\end{document}