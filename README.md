# Language Adversarial Training for Cross-lingual Contextualized Vectors

## Introduction

Implementation of multi-task shared-private model in order to create context dependent cross-lingual representations of sentences.  Model jointly performs BERT's pretraining task for a set of languages and their corresponding corpora.  BERT's masked token prediction task is modified to have a probability of swapping an input token with a context equivalent token in another language.  This is in addition to thhe probability of swapping an input token with the mask token or a random token.

## Usage

### Data generation

Data is generated by sampling long sequences of sentences from various books and documents.  Given a supported corpus, data can be generated by calling the following command

```bash
python crosslingual_bert corpus --options
```

Unsupported corpora can be added by extending the CorpusReader class [details](corpus/README.md).

### Pretraining

Pretraining can be done by calling the following command.

```bash
python crosslingual_bert pretrain --options
```

### Task Evaluation

In progress...

```bash
python crosslingual_bert evaluate --options
```

## Data

Test scripts currently use the czech-english word aligned corpus available [here](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1804).

## Contributing

Extending data generation, corpus readers, and tokenizers. [link](corpus/README.md)

Extending vocab and pytorch datasets. [link](dataset/README.md)

Extending embeddings, attention, and shared-private models. [link](model/README.md)

Extending pretraining tasks and optimizers. [link](trainer/README.md)

## Coding References
[BERT-pytorch](https://github.com/codertimo/BERT-pytorch)
[pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)
